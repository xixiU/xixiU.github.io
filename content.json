{"meta":{"title":"初来的xiaomu","subtitle":null,"description":null,"author":"haha无知","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2019-07-04T12:48:52.211Z","updated":"2017-07-03T12:09:22.000Z","comments":true,"path":"baidu_verify_8ziVdWk6Yo.html","permalink":"http://yoursite.com/baidu_verify_8ziVdWk6Yo.html","excerpt":"","text":"8ziVdWk6Yo"},{"title":"","date":"2019-07-04T12:48:52.211Z","updated":"2017-07-03T12:05:22.000Z","comments":true,"path":"googlef729b6e13d0477cc.html","permalink":"http://yoursite.com/googlef729b6e13d0477cc.html","excerpt":"","text":"google-site-verification: googlef729b6e13d0477cc.html"},{"title":"关于","date":"2017-10-03T05:44:53.000Z","updated":"2019-09-06T05:35:08.187Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"联系方式 Email: &#x76;&#101;&#114;&#x6a;&#x75;&#101;&#64;&#x31;&#x36;&#51;&#46;&#x63;&#111;&#109; GitHub: https://github.com/xixiU 基本信息当前所在城市： 陕西西安 兴趣爱好爬虫、机器学习、深度学习爱好者 站点更新记录2019-09 更新Next ,同时配置字数统计2017-09 常规更新：配置爱心，进度条等2017-06 常规更新：配置站点地图，关于等2017-05 常规更新：多说关闭，使用来必力LiveRe评论系统2017-04 数据清空，全部还原2016-03 常规更新：使用多说评论系统2016-01 开博"},{"title":"categories","date":"2017-10-03T05:52:09.000Z","updated":"2017-10-03T06:56:53.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-10-03T05:43:28.000Z","updated":"2017-10-03T06:56:34.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Java 与Cpp若干比较","slug":"Java 与Cpp若干比较","date":"2019-09-05T16:00:00.000Z","updated":"2019-09-06T06:10:04.267Z","comments":true,"path":"2019/09/06/Java 与Cpp若干比较/","link":"","permalink":"http://yoursite.com/2019/09/06/Java 与Cpp若干比较/","excerpt":"","text":"默认权限的比较 Java Cpp 备注 权限关键字 public、protect、default、private public、protect、friend、private Cpp中friend实际打破了类的封装 默认权限 default(friendly) 仅内部和同包（有继承）可以使用 private，仅内部可以使用，继承类和子类均无法使用 参考链接_带实例 ，参考链接2 对外可见性 Java通过包名定义可见范围，没有全局变量 通过命名空间定义，有全局变量 多态实现 Java中是除了static ,final,private等可以在编译器确定的类别外，其余都是在后期绑定 ，Cpp中是前期绑定。 Java中继承类是父类的一种新类型 Cpp通过virtual 虚函数完成动态绑定 12345678910111213141516171819202122232425262728293031323334353637383940414243class Shape &#123; void draw() &#123; &#125;&#125;class Circle extends Shape &#123; void draw() &#123; System.out.println(\"Circle.draw()\"); &#125;&#125;class Square extends Shape &#123; void draw() &#123; System.out.println(\"Square.draw()\"); &#125;&#125;public class test &#123; public static Shape randShape(int choice) &#123; switch (choice) &#123; default: // To quiet the compiler case 0: return new Circle(); case 1: return new Square(); &#125; &#125; public static void main(String[] args) &#123; Shape[] s = new Shape[2]; // Fill up the array with shapes: for (int i = 0; i &lt; s.length; i++) s[i] = randShape(i); // Make polymorphic method calls: for (int i = 0; i &lt; s.length; i++) s[i].draw(); &#125;&#125; 输出结果12Circle.draw()Square.draw() 可以看出，Java中通过后期绑定的方法，在继承类中改写父类信息，可以直接通过父类访问子类变量和函数。 123456789101112131415161718192021222324252627282930313233class Shape &#123;public: void draw() &#123; cout &lt;&lt; \"Shape draw()\" &lt;&lt; endl; &#125;&#125;;class Circle:public Shape &#123;public: void draw() &#123; cout &lt;&lt; \"Circle.draw()\" &lt;&lt; endl; &#125; &#125;;class Square :public Shape &#123;public: void draw() &#123; cout &lt;&lt; \"Square.draw()\" &lt;&lt; endl; &#125;&#125;;int main() &#123; Shape *s1 =new Circle(),*s2= new Square(); s1-&gt;draw(); s2-&gt;draw(); system(\"pause\"); return 0;&#125; 输出结果12Shape draw()Shape draw() 如果需要完成动态绑定，得到Java程序的输出，需要添加vitrual，将Shape改成如下形式;1234567class Shape &#123;public: virtual void draw() &#123; cout &lt;&lt; \"Shape draw()\" &lt;&lt; endl; &#125;&#125;; 需要注意的是，基于指针的虚函数表，只有存在指针的时候虚函数才会改变指向，如上采用如下方式12345678int main() &#123; Shape s1 =Circle(),s2= Square(); s1.draw(); s2.draw(); system(\"pause\"); return 0;&#125; 永远都是输出Shape的信息,因为s1,s2定义在栈空间，只会存在栈顶指针的移动。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"}]},{"title":"桂林电子科技大学ip出校控制器python登陆","slug":"桂林电子科技大学ip出校控制器批量扫描","date":"2019-07-04T12:48:52.219Z","updated":"2017-10-03T04:07:01.000Z","comments":true,"path":"2019/07/04/桂林电子科技大学ip出校控制器批量扫描/","link":"","permalink":"http://yoursite.com/2019/07/04/桂林电子科技大学ip出校控制器批量扫描/","excerpt":"","text":"2017/10/3卸载了从14年开始使用的py3.4，同时也卸载了pywin插件。 说明本文仅供学习，读者需要对自己行为负责，同时产生的行为与本文无关。读者视作默认同意本说明程序在17年6月开发(ip出校控制器 2.1.4.0版本)，后来只使用过一两次，就没使用过了，也不打算使用和维护了。 开发环境 python (本文使用python3.4) pywin32 (本文使用pywin32 for py3.4) ip出校控制器 2.1.4.0版本对于pywin32的安装，可以点我，下载相应的pywin32版本，然后点击下一步，下一步安装即可。windows句柄适当了解windows句柄有助于程序的移植，点我查看，wikipedia上对windows句柄的介绍 程序中句柄信息，直接通过spy++查找 源码程序思路很简单，就是用py模拟人工输入信息，然后点击按钮。直接上代码；123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# -*- coding: utf-8 -*-\"\"\"Created on Thur June 29 16:00:10 2017@author: x\"\"\"import win32gui,win32api,win32condef find_idxSubHandle(pHandle, winClass, index=0): \"\"\" 已知子窗口的窗体类名 寻找第index号个同类型的兄弟窗口 pHandle父类句柄 winclass同类型句柄，index是同类型该句柄的索引 \"\"\" assert type(index) == int and index &gt;= 0 handle = win32gui.FindWindowEx(pHandle, 0, winClass, None) while index &gt; 0: handle = win32gui.FindWindowEx(pHandle, handle, winClass, None) index -= 1 return handledef find_subHandle(pHandle, winClassList): \"\"\" 递归寻找子窗口的句柄 pHandle是祖父窗口的句柄 winClassList是各个子窗口的class列表，父辈的list-index小于子辈 \"\"\" assert type(winClassList) == list if len(winClassList) == 1: return find_idxSubHandle(pHandle, winClassList[0][0], winClassList[0][1]) else: pHandle = find_idxSubHandle(pHandle, winClassList[0][0], winClassList[0][1]) return find_subHandle(pHandle, winClassList[1:]) def guet_connect(pHandle,idHandle,id,pwHandle,pw,Button_Handle): \"\"\" pHandle为父窗体句柄 idHandle为客户端用户名句柄 pwHandle为客户端密码句柄 Button_Handle为客户端连接按钮句柄 \"\"\" ok_id=win32gui.SendMessage(idHandle, win32con.WM_SETTEXT, None, id) ok_pw=win32gui.SendMessage(pwHandle, win32con.WM_SETTEXT, None, pw) ok_button=win32api.SendMessage(pHandle, win32con.WM_COMMAND, 1, Button_Handle)# 处理之后返回0 if ok_id==0 and ok_pw==0 and ok_button==0: print(\"一次登陆尝试\")def check_connect(pHandle,Button_Handle): \"\"\" pHandle为父窗体句柄 Button_Handle为客户端查询按钮句柄 \"\"\" chaxun_button=win32api.SendMessage(pHandle, win32con.WM_COMMAND, 1, Button_Handle) if chaxun_button==0: ie_error_Handle=win32gui.FindWindow(None,'http://172.16.1.1/ipmanager/login.jsp?id=0 - Internet Explorer')#登陆失败会获取该句柄 ie_ok_Handle=win32gui.FindWindow(None,'网络管理系统 - Internet Explorer')#登陆成功会获取该句柄 if ie_ok_Handle==0: # 登陆成功 print(\"success\") close_ie=win32gui.PostMessage(ie_ok_Handle, win32con.WM_CLOSE, 0, 0) return 1 elif ie_error_Handle==0: #登陆失败 print(\"error\") close_ie=win32gui.PostMessage(ie_error_Handle, win32con.WM_CLOSE, 0, 0) return 0 def write_file(student_id,student_pw): success_passwd=open('ok_passwd.txt','a') success_passwd.write('学号:%s\\t密码:%s\\n'%(student_id,student_pw)) print(\"成功记录一条数据：\"+'学号:%s\\t密码:%s\\n'%(student_id,student_pw)) success_passwd.closedef get_message(hwnd): buf_size = win32gui.SendMessage(hwnd, win32con.WM_GETTEXTLENGTH) + 1 # 要加上截尾的字节 str_buffer = win32gui.PyMakeBuffer(buf_size) # 生成buffer对象 win32api.SendMessage(hwnd, win32con.WM_GETTEXT, buf_size, str_buffer) # 获取buffer # str_buffer = str(str_buffer[:-1]) # 转为字符串 address, length = win32gui.PyGetBufferAddressAndLen(str_buffer) text = win32gui.PyGetString(address, length) return textclass ipGUET(object): def __init__(self, fgFilePath=None): self.Mhandle = win32gui.FindWindow(None,'IP出校控制器') # print (\"IP 出校器初始化完成,父类句柄为%x\"%(self.Mhandle) )# TButton0_handle = find_subHandle(self.Mhandle, [(\"TButton\",0)])# TButton1_handle = find_subHandle(self.Mhandle,[(\"TComboBox\",0),(\"Edit\",0)]) self.TEdit_id_handle = find_subHandle(self.Mhandle, [(\"TEdit\",1)])#用户名 self.TEdit_pw_handle = find_subHandle(self.Mhandle, [(\"TEdit\",0)])#密码 self.TStatusBar_handle = find_subHandle(self.Mhandle, [(\"TStatusBar\",0)])#版本信息 self.TButton_lianjie_handle=win32gui.FindWindowEx(self.Mhandle,0,None,\"连接\") self.TButton_chaxun_handle=win32gui.FindWindowEx(self.Mhandle,0,None,\"查询\") self.GroupBox_handle= find_subHandle(self.Mhandle, [(\"TGroupBox\",0)])#余额 def start(self,student_id,student_pw): guet_connect(self.Mhandle,self.TEdit_id_handle,student_id,self.TEdit_pw_handle,student_pw,self.TButton_lianjie_handle) #print(get_message(TEdit_id_handle)) check_result=check_connect(self.Mhandle,self.TButton_chaxun_handle) if check_result==111: write_file(student_id,student_pw) if __name__==\"__main__\":# win32api.MessageBox(win32con.NULL, 'Python 你好！', '你好', win32con.MB_OK) my_ipguet=ipGUET() my_ipguet.start('你的学号','你的密码') 最后注释已经写好了,程序只供学习","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"虚拟机下caffe与faster-rcnn的only CPU安装","slug":"虚拟机Ubuntu16.04环境下caffe与faster-rcnn的only CPU安装","date":"2019-07-04T12:48:52.219Z","updated":"2017-08-02T12:26:31.000Z","comments":true,"path":"2019/07/04/虚拟机Ubuntu16.04环境下caffe与faster-rcnn的only CPU安装/","link":"","permalink":"http://yoursite.com/2019/07/04/虚拟机Ubuntu16.04环境下caffe与faster-rcnn的only CPU安装/","excerpt":"","text":"虚拟机ubuntu下caffe与faster-rcnn的only CPU安装教程.尝试记录安装过程，给以后配置提供参考主要参考1http://blog.csdn.net/zyb19931130/article/details/53842791主要参考2http://blog.csdn.net/zoro_lov3/article/details/60581174 提前认识硬件环境 RAM :8G CPU: i5-3230M CPU @ 2.60GHz OS:Windows8.1 GPU:GT 740M软件环境（VM虚拟机中） OS:Ubuntu 16.04 安装opencv2.4.10建议给Ubuntu分配不低于3G的内存 一般依赖项安装12345sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install libboost-all-dev sudo apt-get install libatlas-base-devsudo apt-get install python-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev Opencv2.4.10安装文件下载1git clone https://github.com/jayrambhia/Install-OpenCV 安装Opencv依赖项文件dependencies.sh在Ubuntu目录12cd Ubuntusudo ./dependencies.sh 安装Opencv2.4.10123cd 2.4sudo chmod +x *.sh sudo sh ./opencv2_4_10.sh Opencv安装比较久，安装完成之后会显示OpenCV 2.4.10 ready to be used Opencv测试 切换到opencv下载解压后的文件夹目录下，然后进入sample/c/目录下，编译样例文件，具体如下：cd ~/下载/opencv-2.4.10/samples/c/ 如下载在Ubuntu/2.4/Opencv中，则输入 1234cd ~/Install-Opencv/Ubuntu/2.4/Opencv/opencv-2.4.10/samples/c/./build_all.sh 执行完成后，会生成对应的可执行文件 运行其中一个样例,如 1./find_ob 显示图像则为正确安装 caffe安装下载caffe12cd ~ git clone --recursive https://github.com/BVLC/caffe.git 复制配置文件12cd caffecp Makefile.config.example Makefile.config 修改配置文件1gedit Makefile.config ctrl +F找到以下几行,并做如下修改12345678910111213去掉注释CPU_ONLY :=1 注释掉CUDA有关的行： #CUDA_DIR := /usr/local/cuda #CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ # -gencode arch=compute_30,code=sm_30 \\ # -gencode arch=compute_35,code=sm_35 \\ # -gencode arch=compute_50,code=sm_50 \\ # -gencode arch=compute_50,code=compute_50 去掉注释WITH_PYTHON_LAYER := 1 这一行有所改动INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial 这一行有所改动LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/i386-linux-gnu/hdf5/serial /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial 注释掉这一行#TEST_GPUID := 0 编译文件1234make all make test make runtest make pycaffe 若在编译过程中，没有报错 ，则进入下一步 尝试import caffe123$cd caffe/python python &gt;&gt;&gt;import caffe 如果没有报错，需要继续输入1dir(caffe) 如果显示123456['AdaDeltaSolver', 'AdaGradSolver', 'AdamSolver', 'Classifier', 'Detector', 'Layer', 'NesterovSolver', 'Net', 'NetSpec', 'RMSPropSolver', 'SGDSolver', 'TEST', 'TRAIN','__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', '__version__', '_caffe', 'classifier', 'detector', 'get_solver', 'io', 'layer_type_list','layers', 'net_spec', 'params', 'proto', 'pycaffe', 'set_device', 'set_mode_cpu', 'set_mode_gpu','set_random_seed', 'to_proto'] 说明安装成功。如果报错，一般是部分依赖项没有安装上，请参考最开始的一般依赖项检查。 py-faster-rcnn的CPU安装py-faster-rcnn下载12cd ~ git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 安装安装cython和easydict12sudo pip install cython sudo pip install easydict 编译cython并做如下修改12cd py-faster-rcnn/libgedit setup.py crtl+f查找，将setup.py文件中部分行 ，修改为如下所示12345678910111213141516171819#CUDA = locate_cuda() #self.set_executable('compiler_so', CUDA['nvcc']) #Extension('nms.gpu_nms', #['nms/nms_kernel.cu', 'nms/gpu_nms.pyx'], #library_dirs=[CUDA['lib64']], #libraries=['cudart'], #language='c++', #runtime_library_dirs=[CUDA['lib64']], # this syntax is specific to this build system # we're only going to use certain compiler args with nvcc and not with # gcc the implementation of this trick is in customize_compiler() below #extra_compile_args=&#123;'gcc': [\"-Wno-unused-function\"], # 'nvcc': ['-arch=sm_35', # '--ptxas-options=-v', # '-c', # '--compiler-options', # \"'-fPIC'\"]&#125;, #include_dirs = [numpy_include, CUDA['include']] #), 编译 文件1make 修改并编译caffe 直接到之前的caffe目录，找到Makefile.config，然后复制粘贴到/py-faster-rcnn/caffe-fast-rcnn/ 打开CMakeLists.txt，做如下修改（OFF改成ON） 1caffe_option(CPU_ONLY &quot;Build Caffe without CUDA support&quot; ON) # TODO: rename to USE_CUDA 编译cafe 12cd ~/py-faster-rcnn/caffe-fast-rcnn make -j8&amp;&amp; make pycaffe 下载测试文件 采用ubuntu下载 12cd ~/py-faster-rcnn ./data/scripts/fetch_faster_rcnn_models.sh 外部下载工具下载，然后移动到py-faster-rcnn/data中下载链接https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0 下载完成之后 ，解压1tar zxvf faster_rcnn_models.tgz -C ./ 修改部分文件 修改/py-faster-rcnn/lib/fast_rcnn/config.py文件（True改成False） 12# Use GPU implementation of non-maximum suppression__C.USE_GPU_NMS = False 将/py-faster-rcnn/tools/test_net.py和 /py-faster-rcnn/tools/train_net.py的caffe.set_mode_gpu()修改为caffe.set_mode_cpu(). 修改/py-faster-rcnn/lib/fast_rcnn/nms_wrapper.py文件（注释该引用，并将False改成True） 12#from nms.gpu_nms import gpu_nmsdef nms(dets, thresh, force_cpu=True): 运行demo12cd ~/py-faster-rcnn sudo ./tools/demo.py --cpu 如果显示123syncedmem.hpp:25] Check failed: *ptr host allocation of size 345600000 failed*** Check failure stack trace: ***已放弃 (核心已转储) 个人猜测是内存不足的原因，如有其他解决方案，烦请评论提醒。此处尝试更换测试数据集，如下12cd ~/py-faster-rcnn sudo ./tools/demo.py --cpu --net zf 稍等片刻既可 最后清理系统垃圾在安装过程中，可能产生系统垃圾或者不需要的软件，可以将其清理掉点这里参考这篇博客解决","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/categories/deep-learning/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"rcnn","slug":"rcnn","permalink":"http://yoursite.com/tags/rcnn/"}]},{"title":"iccv2017文章批量下载","slug":"iccv2017","date":"2017-11-17T16:00:00.000Z","updated":"2017-11-20T07:37:18.000Z","comments":true,"path":"2017/11/18/iccv2017/","link":"","permalink":"http://yoursite.com/2017/11/18/iccv2017/","excerpt":"","text":"说明完整下载文件使用后台运行程序，单线程运行大概10h，多线程大概运行4h58分，都下载了621个pdf(ls -l|grep “^-“|wc -l),一共1.5G(du -h).先贴出完整运行文件， 点我下载单线程版 点我下载多线程版功能逻辑从http://openaccess.thecvf.com/ICCV2017.py下载iccv2017的所有论文，以及整理生成其bibtex的文献格式。 程序实现逻辑比较简单： 正则表达式抓关键词 生成文本，保存所有bibtex文献信息 逐个下载(单线程)/线程并发(多线程) 在和程序同目录下会生成一个文档(默认 bibref.txt)及文件夹(默认iccv)，分别存放bibtex信息和所有下载的pdf，下面贴出完整代码和下载文件 程序中添加了代理等，按照需要修改即可。 单线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# -*- coding: utf-8 -*-\"\"\"Created on Mon Oct 30 11:56:56 2017@author: xBlog:xixiu.github.io\"\"\"import urllib.requestimport time,datetimeimport osimport socketimport reimport http.clientclass Iccv_rawler(object): # 睡眠时长 __time_sleep = 0.1 #默认下载文件夹 __mydir ='iccv2017' __i_headers=&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.101 Safari/537.36',&#125; __proxy=False __url='http://openaccess.thecvf.com/ICCV2017.py' __paperlist=[] __biblist=[] original_fileneme='bibref.txt' # t 下载图片时间间隔 def __init__(self, t=0.1,mydir='iccv2017'): self.time_sleep = t self.__mydir='./' + mydir + '/' def pageget(self): print('begin to getpage') #设置代理 if(self.__proxy): proxy = &#123;'http':'127.0.0.1:12306'&#125; proxy_support = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_support,urllib.request.HTTPHandler(debuglevel=1)) urllib.request.install_opener(opener) try: req=urllib.request.Request(url=self.__url,headers=self.__i_headers) page=urllib.request.urlopen(req)# data=page.read().decode('utf8')#ISO-8859-1 data=page.read().decode('utf8') except http.client.IncompleteRead as icread: data=icread.partial.decode('utf-8') except urllib.error.URLError as e: print(\"-----urlErrorurl:\", self.__url) pass except socket.timeout as e: print(\"-----socket timout:\", self.__url) else: print('1') finally: page.close() self.get_keyword(data) self.write_bib() self.__downloadPdf() def __downloadPdf_single(self, single_info,i): time.sleep(self.time_sleep) name = lambda x:x.split('/')[-1] try: if not os.path.isdir(self.__mydir): os.makedirs(self.__mydir ) lengthpap=len(self.__paperlist) print('TIme:%s begin to download the %d -th ,left %d to download .\\nThe name of the pdf is %s \\n'%(time.ctime(time.time()),i,lengthpap-i,name(single_info))) urllib.request.urlretrieve('http://openaccess.thecvf.com/content_ICCV_2017/papers'+single_info ,self.__mydir + name(single_info)) except Exception as ex: template = \"An exception of type &#123;0&#125; occurred. Arguments:\\n&#123;1!r&#125;\" message = template.format(type(ex).__name__, ex.args) print (message) def write_bib(self): with open(self.original_fileneme,'w',encoding='utf-8') as f: for x in self.__biblist: f.write(str(x).replace('&lt;br&gt;') ,'') f.close print('write bib ref success!Please chech in the progaram dir\\n') def get_keyword(self,data): #在pattern中需要去掉()，/ print('begin to filter') pattern_bibref=r'&lt;div class=\"bibref\"&gt;(.*?)&lt;/div&gt;' self.__biblist=re.compile(pattern_bibref,re.DOTALL).findall(data)#len(keyword_list) 619 pattern_pdf=r'&lt;a href=\"content_ICCV_2017/papers(.*?)\"&gt;pdf&lt;/a&gt;' self.__paperlist=re.compile(pattern_pdf,re.DOTALL).findall(data)#618 # 下载 def __downloadPdf(self): i=1 for x in self.__paperlist: self.__downloadPdf_single(x,i) i+=1 def start(self): start_time=time.time() self.pageget() print(\"All done\\n---Time use %s ---\" %str(datetime.timedelta(seconds=int(time.time()-start_time))))if __name__ == '__main__': myiccv=Iccv_rawler() myiccv.start() 多线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# -*- coding: utf-8 -*-\"\"\"Created on Fri Nov 17 19:09:04 2017@author: xBlog:xixiu.github.io\"\"\"import urllib.requestimport time,datetimeimport osimport socketimport reimport http.clientimport threadingclass Iccv_rawler(object): # 睡眠时长 __time_sleep = 0.1 #默认下载文件夹 __mydir ='iccv2017' __i_headers=&#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.101 Safari/537.36',&#125; __proxy=False __url='http://openaccess.thecvf.com/ICCV2017.py' __paperlist=[] __biblist=[] original_fileneme='bibref.txt' # t 下载图片时间间隔 def __init__(self, t=0.1,mydir='iccv2017'): self.time_sleep = t self.__mydir='./' + mydir + '/' def pageget(self): print('begin to getpage') #设置代理 if(self.__proxy): proxy = &#123;'http':'127.0.0.1:12306'&#125; proxy_support = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_support,urllib.request.HTTPHandler(debuglevel=1)) urllib.request.install_opener(opener) try: req=urllib.request.Request(url=self.__url,headers=self.__i_headers) page=urllib.request.urlopen(req)# data=page.read().decode('utf8')#ISO-8859-1 data=page.read().decode('utf8') except http.client.IncompleteRead as icread: data=icread.partial.decode('utf-8') except urllib.error.URLError as e: print(\"-----urlErrorurl:\", self.__url) pass except socket.timeout as e: print(\"-----socket timout:\", self.__url) else: print('1') finally: page.close() self.get_keyword(data) self.write_bib() self.__downloadPdf() def __downloadPdf_single(self, single_info,i): time.sleep(self.time_sleep) name = lambda x:x.split('/')[-1] try: if not os.path.isdir(self.__mydir): os.makedirs(self.__mydir ) lengthpap=len(self.__paperlist) print('TIme:%s begin to download the %d -th ,left %d to download .\\nThe name of the pdf is %s \\n'%(time.ctime(time.time()),i,lengthpap-i,name(single_info))) urllib.request.urlretrieve('http://openaccess.thecvf.com/content_ICCV_2017/papers'+single_info ,self.__mydir + name(single_info)) except Exception as ex: template = \"An exception of type &#123;0&#125; occurred. Arguments:\\n&#123;1!r&#125;\" message = template.format(type(ex).__name__, ex.args) print (message) def write_bib(self): with open(self.original_fileneme,'w',encoding='utf-8') as f: for x in self.__biblist: f.write(str(x).replace('&lt;br&gt;','') ) f.close print('write bib ref success!Please chech in the progaram dir\\n') def get_keyword(self,data): #在pattern中需要去掉()，/ print('begin to filter') pattern_bibref=r'&lt;div class=\"bibref\"&gt;(.*?)&lt;/div&gt;' self.__biblist=re.compile(pattern_bibref,re.DOTALL).findall(data)#len(keyword_list) 619 pattern_pdf=r'&lt;a href=\"content_ICCV_2017/papers(.*?)\"&gt;pdf&lt;/a&gt;' self.__paperlist=re.compile(pattern_pdf,re.DOTALL).findall(data)#618 # 下载 def __downloadPdf(self): i=1 for x in self.__paperlist: one_thr = threading.Thread(target=self.__downloadPdf_single, args=[x,i]) one_thr.start() one_thr.join() i+=1# self.__downloadPdf_single(x) def start(self): start_time=time.time() self.pageget() print(\"All done\\n---Time use %s ---\" %str(datetime.timedelta(seconds=int(time.time()-start_time)))) if __name__ == '__main__': myiccv=Iccv_rawler() myiccv.start()","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"MySQL进阶学习笔记","slug":"MySQL进阶","date":"2017-10-21T16:00:00.000Z","updated":"2019-09-06T05:51:52.451Z","comments":true,"path":"2017/10/22/MySQL进阶/","link":"","permalink":"http://yoursite.com/2017/10/22/MySQL进阶/","excerpt":"","text":"之前只是简单得学习了MySQL的增删查改等，在项目中深感知识不够，于是学习了一下MySQL触发器，存储过程，权限管理等知识。 触发器是什么触发器就是对有关系的数据表进行操作的一种功能，当满足触发条件，则执行指定的事件。比如商品的库存表和用户的订单表之间，若用户下单，用户订单表会相应增加，而库存表则会相应减少，触发器就可以完成这样的操作。如果不实用触发器，则需要在事件之后，对有关联的数据表手动update。如上订单表与库存表，用户下单后，需要手动更新库存表。 组成触发器的组成包括触发对象(谁)，触发事件(做了什么事)，触发事件(时间)，触发之后执行的事件(因前面的做了什么事，需要做什么) 语法123456create trigger &lt;触发器名称&gt; --触发器`必须`有名字，最多64个字符&#123; before | after &#125; --触发事件，决定满足条件之后的执行语句，在该条件之前还是之后执行&#123; insert | update | delete &#125; --指定触发事件。on &lt;表名称&gt; --触发器是属于某一个表的:当在这个表上执行插入、 更新或删除操作的时候就导致触发器的激活. 我们不能给同一张表的同一个事件安排两个触发器。for each row --触发器的执行间隔：FOR EACH ROW子句通知触发器 每隔一行执行一次动作，而不是对整个表执行一次。&lt;满足条件之后，需要执行的SQL语句&gt; --触发器包含所要触发的SQL语句，可以是复合语句。 例如12345678910delimiter $ --修改结束符，以免和SQL语句结束符混淆create trigger t1beforeupdateon ordefor each rowbeginupdate goods set num = num+old.much-new.much where gid=old.gid;end$delimiter ; 注意：不能同时在一个表上建立2个相同类型的触发器 new &amp; old触发器语句中会涉及到参数传递例如触发条件是delete，则在语句之后之后，会有一条语句被删除，因此若在语句之后执行触发事件，并且需要引用之前的数据，可以采用old这个关键词。又如触发条件是insert，则在语句执行之后，会有一条语句被添加，因此若在语句之后执行触发事件，并且需要引用之前的数据，可以采用new这个关键词。对于update而言，一般new 与old都同时存在。注意触发事件before与after的选择：不能在触发之后对之前的数据进行更新 实例因为触发器是需要指定表的，因此需要指定数据库。以库存表与用户订单表为例 新建数据库与表新建表并且随便指定一部分数据 goods为库存表(gid库存标号,name名称,num数量)，orde为订单表(oid订单编号,gid库存编号,much购买商品数目) 12345678910111213create database learntrigger;CREATE TABLE `goods` ( `gid` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, `num` smallint(6) DEFAULT NULL) ;CREATE TABLE `orde` ( `oid` int(11) DEFAULT NULL, `gid` int(11) DEFAULT NULL, `much` smallint(6) DEFAULT NULL); 新建一个触发器触发器完成如下操作：用户下单之后，库存相应减少. 123456789delimiter $ create trigger t1beforeinserton ordefor each rowbeginupdate goods set num:=num-new.much where gid=new.gid;end$ 完善上述触发器：若用户下单数目，超过库存，则将用户下单的数量自动设置为库存数，以免出现库存为负数的情形123456789101112131415delimiter $ create trigger t1beforeinserton ordefor each rowbegindeclaretemp_num int;select num into temp_num from goods where gid = new.gid;if new.much &gt; temp_num thenset new.much=temp_num;end if;update goods set num:=num-new.much where gid=new.gid;end$ 这里必须选择before，若为update因为insert之后数据已经生成，这个时候确再去修改insert的值，存在逻辑错误。 其他 查看触发器 1show triggers [FROM database_name]; 若有先使用use database_name;则可以不添加[FROM database_name] 添加\\G 参数可以以列的形式显示，如show triggers \\G; 删除触发器1drop trigger [IF EXISTS] [database_name.]trigger_name 存储过程与游标与触发器比较存储过程与触发器类似，但是存储过程可以主动调用。参数传递更加灵活，功能增加强大。 组成1234create procedure procedure_name([paraname paratype])begin&lt;sql语句&gt;end 调用call procedure_name([paraname paratype]);paraname 为用户指定的参数名称，paratype可以从in out inout选择in表示输入型参数，out表示输出型参数，inout表示可进可出 实例以下创建一个输入矩形的宽高，输出矩阵面积，并且判断矩形的宽与高的关系123456789101112delimiter $create procedure p100(width int,height int)beginselect concat('面积',width*height) as area;if width&gt;height thenselect \"宽大于高\";elseif width &lt; height thenselect \"高大于宽\";else select \"正方形\";end if;end$ 调用1call p1(100,200)$ 权限管理参考文献： 英文参考文献 中文参考文献MySQL提供的权限管理，分为连接的权利与执行的权利.连接的权利连接的权利指的是：是否有权限连接MySQL,该部分又MYSQL数据库下面的mysql数据库中的user表段控制。12345mysql -uroot -pshow databases; /*此处可以看到有一个mysql数据库*/use mysql;select host ,user,password from user;/*查看可以有效连接的用户信息*//*host从哪里来、用户名username、密码possword*/ 从上面可以看到对权限的管理主要从user,password,host三个参数来实现控制 执行的权利连接上数据库之后，执行SQL语句时还需要进行权限检查。包括执行权限(增删查改)，授予的权限(全局层级,数据库层级,表层级,列层级,子程序层级)的检查 新建具有特定权限的用户grand [权限1,选项2,权限3……] on . to username@’set_host’ identified by ‘set_password’；/ 授予权限/recoke [权限1,选项2,权限3……] on . to username@’set_host’ identified by ‘set_password’；/ 取消权限/其中*.*对应上面授予的权限层级的不同有不同的写法。 全局层级全局权限适用于一个给定服务器中的所有数据库。这些权限存储在mysql.user表中。GRANT ALL ON .和REVOKE ALL ON .只授予和撤销全局权限。 数据库层级数据库权限适用于一个给定数据库中的所有目标。这些权限存储在mysql.db和mysql.host表中。GRANT ALL ON db_name.和REVOKE ALL ON db_name.只授予和撤销数据库权限。 表层级表权限适用于一个给定表中的所有列。这些权限存储在mysql.talbes_priv表中。GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。 列层级列权限适用于一个给定表中的单一列。这些权限存储在mysql.columns_priv表中。当使用REVOKE时，您必须指定与被授权列相同的列。 子程序层级CREATE ROUTINE, ALTER ROUTINE, EXECUTE和GRANT权限适用于已存储的子程序。这些权限可以被授予为全局层级和数据库层级。而且，除了CREATE ROUTINE外，这些权限可以被授予为子程序层级，并存储在mysql.procs_priv表中。 权限 意义 ALL [PRIVILEGES] 设置除GRANT OPTION之外的所有简单权限 ALTER 允许使用ALTER TABLE ALTER ROUTINE 更改或取消已存储的子程序 CREATE 允许使用CREATE TABLE CREATE ROUTINE 创建已存储的子程序 CREATE TEMPORARY TABLES 允许使用CREATE TEMPORARY TABLE CREATE USER 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。 CREATE VIEW 允许使用CREATE VIEW DELETE 允许使用DELETE DROP 允许使用DROP TABLE EXECUTE 允许用户运行已存储的子程序 FILE 允许使用SELECT…INTO OUTFILE和LOAD DATA INFILE INDEX 允许使用CREATE INDEX和DROP INDEX INSERT 允许使用INSERT LOCK TABLES 允许对您拥有SELECT权限的表使用LOCK TABLES PROCESS 允许使用SHOW FULL PROCESSLIST REFERENCES 未被实施 RELOAD 允许使用FLUSH REPLICATION CLIENT 允许用户询问从属服务器或主服务器的地址 REPLICATION SLAVE 用于复制型从属服务器（从主服务器中读取二进制日志事件） SELECT 允许使用SELECT SHOW DATABASES SHOW DATABASES显示所有数据库 SHOW VIEW 允许使用SHOW CREATE VIEW SHUTDOWN 允许使用mysqladmin shutdown SUPER 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。 UPDATE 允许使用UPDATE USAGE “无权限”的同义词 GRANT OPTION 允许授予权限 注意设置之后需要冲刷权限flush privileges;","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/tags/Mysql/"}]},{"title":"Sklearn学习笔记","slug":"sklearn学习笔记","date":"2017-10-21T16:00:00.000Z","updated":"2019-09-06T05:51:54.847Z","comments":true,"path":"2017/10/22/sklearn学习笔记/","link":"","permalink":"http://yoursite.com/2017/10/22/sklearn学习笔记/","excerpt":"","text":"参考:sklearnm官方文档 Sklearn 简介sklearn 是什么python机器学习的包 安装使用pippip install scikit-learnsudo python3 -m pip install scikit-learn 使用Anacondaconda install scikit-learn sklearn学习方法流程图 常用属性/方法data 训练数据，一般用train来表示 训练数据的分类属性，一般用target来表示 测试数据，一般用test来表示 测试数据的真实分类属性，用于评估分类器性能，一般用expected来表示 model 学习数据，一般用fit() 预测数据，一般用predict() 评价模型准确度，一般用score() Example 演示数据类型 123456#以下程序将输出sklearn库中内置的bosten数据from sklearn import datasetshouse_test=datasets.load_boston()print(house_test.data[:20])print(house_test.target[:20]) 使用系统数据和模型预测示例 12345678from sklearn.model_selection import train_test_splitfrom sklearn import linear_modellr = linear_model.LinearRegression()house_test=datasets.load_boston()X_train, X_test, y_train, y_test = train_test_split(house_test.data, house_test.target, test_size=0.3)lr.fit(X_train, y_train)print(lr.predict(X_test)[:10])print(y_test[:10]) 标准化/归一化/正则化标准化/规范化概念数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。由于信用指标体系的各个指标度量单位是不同的，为了能够将指标参与评价计算，需要对指标进行规范化处理，通过函数变换将其数值映射到某个数值区间。 算法 z-score标准化(或零－均值标准化)（常用） y=(x-X的平均值)／X的标准差=(x-mean)/std优点：当X的最大值和最小值未知，或孤立点左右了最大－最小规范化时，该方法有用 最小－最大规范化(线性变换)y=( (x-MinValue) / (MaxValue-MinValue) )(new_MaxValue-new_MinValue)+new_minValue 小数定标规范化：通过移动X的小数位置来进行规范化 y= x/10的j次方 (其中,j使得Max(|y|) &lt;1的最小整数 对数Logistic模式： 新数据=1/（1+e^(-原数据)） 模糊量化模式： 新数据=1/2+1/2sin[派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） ] X为原数据 示例 使用sklearn.preprocessing.scale() 可以直接将给定数据进行标准化 123456789101112#经常操作的参数为axis，以m * n矩阵举例：#axis 不设置值，对 m*n 个数求均值，返回一个实数#axis = 0：压缩行，对各列求均值，返回 1* n 矩阵#axis =1 ：压缩列，对各行求均值，返回 m *1 矩阵from sklearn import preprocessingimport numpy as npX = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])print(X.mean(axis=0))X_scaled = preprocessing.scale(X)print(X_scaled.mean(axis=0)) 标准化对准确率的影响 12345678910111213141516from __future__ import print_functionfrom sklearn import preprocessingfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets.samples_generator import make_classificationfrom sklearn.svm import SVCimport matplotlib.pyplot as plt X, y = make_classification(n_samples=300, n_features=2 , n_redundant=0, n_informative=2,random_state=22, n_clusters_per_class=1, scale=100)plt.scatter(X[:, 0], X[:, 1], c=y)plt.show()X = preprocessing.scale(X) # normalization stepX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)clf = SVC()clf.fit(X_train, y_train)print(clf.score(X_test, y_test)) 归一化概念 把数变为（0，1）之间的小数主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。 把有量纲表达式变为无量纲表达式归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 算法 线性转换 y=(x-MinValue)/(MaxValue-MinValue) 对数函数转换： y=log10(x) 反余切函数转换 y=atan(x)*2/PI 线性也与对数函数结合 式(1)将输入值换算为[-1,1]区间的值， 在输出层用式(2)换算回初始值，其中和分别表示训练样本集中负荷的最大值和最小值 正则化概念正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。 Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。 p-范数的计算公式：||X||p=(|x1|^p+|x2|^p+…+|xn|^p)^1/p 该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。 在sklearn中 可以使用preprocessing.normalize()函数对指定数据进行转换 processing.Normalizer()类实现对训练集和测试集的拟合和转换 Cross-validation概念cross validation大概的意思是：对于原始数据我们要将其一部分分为train_data，一部分分为test_data。train_data用于训练，test_data用于测试准确率。在test_data上测试的结果叫做validation_error。将一个算法作用于一个原始数据，我们不可能只做出随机的划分一次train和test_data，然后得到一个validation_error，就作为衡量这个算法好坏的标准。因为这样存在偶然性。我们必须好多次的随机的划分train_data和test_data，分别在其上面算出各自的validation_error。这样就有一组validation_error，根据这一组validation_error，就可以较好的准确的衡量算法的好坏 在sklearn中sklearn中的cross validation模块，最主要的函数是如下函数：sklearn.cross_validation.cross_val_score:他的调用形式是scores = cross_validation.cross_val_score(clf, raw_data, raw_target, cv=5, score_func=None)参数解释：clf:表示的是不同的分类器，可以是任何的分类器。比如支持向量机分类器。clf = svm.SVC(kernel=’linear’, C=1)；raw_data：原始数据；raw_target:原始类别标号；cv：代表的就是不同的cross validation的方法了。引用scikit-learn上的一句话（When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.）如果cv是一个int数字的话，那么默认使用的是KFold或者StratifiedKFold交叉，如果如果指定了类别标签则使用的是StratifiedKFold。cross_val_score:这个函数的返回值就是对于每次不同的的划分raw_data时，在test_data上得到的分类的准确率。至于准确率的算法可以通过score_func参数指定，如果不指定的话，是用clf默认自带的准确率算法。 示例 使用系统默认数据集演示交叉验证 12345678910111213141516171819202122232425from sklearn.datasets import load_iris # iris数据集from sklearn.model_selection import train_test_split # 分割数据模块from sklearn.neighbors import KNeighborsClassifier # K最近邻(kNN，k-NearestNeighbor)分类算法from sklearn.cross_validation import cross_val_score # K折交叉验证模块#加载iris数据集iris = load_iris()X = iris.datay = iris.target#分割数据并X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)#建立模型knn = KNeighborsClassifier()#训练模型knn.fit(X_train, y_train)#使用K折交叉验证模块scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')print(scores)#print(scores.mean())#将准确率打印出print(knn.score(X_test, y_test)) 为模交叉验证选择合适的参数 123456789101112131415161718192021222324252627282930313233343536from sklearn.datasets import load_iris # iris数据集from sklearn.neighbors import KNeighborsClassifier # K最近邻(kNN，k-NearestNeighbor)分类算法from sklearn.cross_validation import cross_val_score # K折交叉验证模块import matplotlib.pyplot as plt#加载iris数据集iris = load_iris()X = iris.datay = iris.targetk_range=range(1,10)k_loss=[]k_score=[]for k in k_range: #建立模型 knn = KNeighborsClassifier(n_neighbors=k) #使用K折交叉验证模块 scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy') loss = -cross_val_score(knn, X, y, cv=5, scoring='neg_mean_squared_error')#'neg_mean_squared_error' 'mean_squared_error' k_loss.append(loss.mean()) k_score.append(scores.mean()) #add mean of the scores plt.figure(1)plt.subplot(121)plt.plot(k_range,k_loss)plt.xlabel('Value from loss ,the best k of KNN id %d'%(k_loss.index(min(k_loss))))plt.ylabel('Error')#plt.figure(2)plt.subplot(122)plt.plot(k_range,k_score)plt.xlabel('Value from loss ,the best k of KNN id %d'%(k_score.index(min(k_score))))plt.ylabel('score')plt.show() curve fit参数选择中，或许会存在过拟合与欠拟合的问题，给模型选择合适参数，对结果准确率起着至关重要的作用。举栗子 可视化特定gamma参数对SVC的影响 设置gamma=0.01 123456789101112131415161718192021222324from sklearn.learning_curve import learning_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npdigits=load_digits()X=digits.dataY=digits.targettrain_sizes, train_loss, test_loss= learning_curve(SVC(gamma=0.01), X, Y,cv=10,scoring='neg_mean_squared_error',train_sizes=[0.1, 0.25, 0.5, 0.75, 1])train_loss_mean = -np.mean(train_loss, axis=1)test_loss_mean = -np.mean(test_loss, axis=1) plt.plot(train_sizes, train_loss_mean, 'o-', color=\"r\",label=\"Training\")plt.plot(train_sizes, test_loss_mean, 'o-', color=\"g\",label=\"Test-Cross-validation\") plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")plt.legend(loc=\"best\")plt.show() 设置gamma=0.001 123456789101112131415161718192021222324from sklearn.learning_curve import learning_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npdigits=load_digits()X=digits.dataY=digits.targettrain_sizes, train_loss, test_loss= learning_curve(SVC(gamma=0.001), X, Y,cv=10,scoring='neg_mean_squared_error',train_sizes=[0.1, 0.25, 0.5, 0.75, 1])train_loss_mean = -np.mean(train_loss, axis=1)test_loss_mean = -np.mean(test_loss, axis=1) plt.plot(train_sizes, train_loss_mean, 'o-', color=\"r\",label=\"Training\")plt.plot(train_sizes, test_loss_mean, 'o-', color=\"g\",label=\"Test-Cross-validation\") plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")plt.legend(loc=\"best\")plt.show() 给模型选择合适的参数 123456789101112131415161718192021222324from sklearn.learning_curve import validation_curvefrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npdigits=load_digits()X=digits.dataY=digits.targetparam_range=np.logspace(-6,-2.3,5)train_loss, test_loss= validation_curve(SVC(), X, Y,param_name='gamma',param_range=param_range,cv=10,scoring='neg_mean_squared_error')train_loss_mean = -np.mean(train_loss, axis=1)test_loss_mean = -np.mean(test_loss, axis=1) plt.plot(param_range, train_loss_mean, 'o-', color=\"r\",label=\"Training\")plt.plot(param_range, test_loss_mean, 'o-', color=\"g\",label=\"Test-Cross-validation\") plt.xlabel(\"gamma\")plt.ylabel(\"Loss\")plt.legend(loc=\"best\")plt.show()","categories":[{"name":"deep learning Analysis","slug":"deep-learning-Analysis","permalink":"http://yoursite.com/categories/deep-learning-Analysis/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/tags/deep-learning/"}]},{"title":"400w条QQ空间爬虫，及好友网络分析","slug":"qnetwork","date":"2017-10-03T16:00:00.000Z","updated":"2017-10-07T05:09:14.000Z","comments":true,"path":"2017/10/04/qnetwork/","link":"","permalink":"http://yoursite.com/2017/10/04/qnetwork/","excerpt":"","text":"学习机器学习算法，手上没有数据，于是决定自己动手，爬取QQ空间数据本文参考文献 Python3.6获取QQ空间全部好友列表 Practical Data Science Cookbook .Tony Ojeda Sean, Patrick Murphy,Benjamin Bengfort ,Abhiji Dasgupta PACKT PUBLISHING 结果只举例好友网络，说说时间，过多信息涉及好友隐私 数据说明:为防止腾讯检测，随机丢掉一部分数据。两代运行之后，大概有400w左右数据(Aliyun运行了大概8个小时) 好友网络整张网络结构：过大，根本看不出什么以某一好友为例，为保护隐私，数据标签不显示 hops表示最多经过几个节点(人) 发说说时间随机抽取10w数据，统计。可以从下面（特别是小时统计）看出，在网络中的作息规律。随机抽取100w数据，然后从中选出17年的80223条数据，单独分析每个月的情况10w数据随机选择，然后17年只有1-9月的数据 按年统计 按月统计基本上平均 按天统计 每小时统计图 部分源码python登陆QQ空间version 1 将自己的账号密码写入一个文本(userinfo.ini)，时候操作过快会提示错误userinfo.ini内容[qq_info]qq_number=xxx(你的qq号)qq_password=(你的密码) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#coding:utf-8from selenium import webdriverimport requestsimport timeimport osfrom urllib import parseimport configparserclass Spider(object): def __init__(self): self.web=webdriver.Firefox() self.web.get('https://user.qzone.qq.com') config = configparser.ConfigParser(allow_no_value=False) config.read('userinfo.ini') self.__username =config.get('qq_info','qq_number') #self.__password=config.get('qq_info','qq_password') self.headers=&#123; 'host': 'h5.qzone.qq.com', 'accept-encoding':'gzip, deflate, br', 'accept-language':'zh-CN,zh;q=0.8', 'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'user-agent':'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:55.0) Gecko/20100101 Firefox/55.0', 'connection': 'keep-alive' &#125; self.req=requests.Session() self.cookies=&#123;&#125; def login(self): self.web.switch_to_frame('login_frame') log=self.web.find_element_by_id(\"switcher_plogin\") log.click() time.sleep(1) username=self.web.find_element_by_id('u') username.send_keys(self.__username) ps=self.web.find_element_by_id('p') ps.send_keys(self.__password) btn=self.web.find_element_by_id('login_button') time.sleep(5)#延时太小会被检测，然后报错 self.web.get('https://user.qzone.qq.com/&#123;&#125;'.format(self.__username)) cookie='' for elem in self.web.get_cookies(): cookie+=elem[\"name\"]+\"=\"+ elem[\"value\"]+\";\" self.cookies=cookie self.get_g_tk() #time.sleep(10) self.headers['Cookie']=self.cookies self.web.quit() def get_g_tk(self): p_skey = self.cookies[self.cookies.find('p_skey=')+7: self.cookies.find(';', self.cookies.find('p_skey='))] h=5381 for i in p_skey: h+=(h&lt;&lt;5)+ord(i) print('g_tk',h&amp;2147483647) self.g_tk=h&amp;2147483647 if __name__=='__main__': sp=Spider() sp.login() version 2自己扫码登陆。注意需要等待控制台提示再操作，不然selenium抓不到窗口1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#coding:utf-8import requestsimport timeimport osfrom urllib import parseclass Spider(object): def __init__(self): self.web=webdriver.Firefox() self.web.get(&apos;https://user.qzone.qq.com&apos;) self.__username =&apos;这里填你的QQ号&apos;#后续操作会用到 self.headers=&#123; &apos;host&apos;: &apos;h5.qzone.qq.com&apos;, &apos;accept-encoding&apos;:&apos;gzip, deflate, br&apos;, &apos;accept-language&apos;:&apos;zh-CN,zh;q=0.8&apos;, &apos;accept&apos;:&apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;, &apos;user-agent&apos;:&apos;Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:55.0) Gecko/20100101 Firefox/55.0&apos;, &apos;connection&apos;: &apos;keep-alive&apos; &#125; self.req=requests.Session() self.cookies=&#123;&#125; def get_g_tk(self): p_skey = self.cookies[self.cookies.find(&apos;p_skey=&apos;)+7: self.cookies.find(&apos;;&apos;, self.cookies.find(&apos;p_skey=&apos;))] h=5381 for i in p_skey: h+=(h&lt;&lt;5)+ord(i) print(&apos;g_tk&apos;,h&amp;2147483647) self.g_tk=h&amp;2147483647 def login(self): print(&apos;请扫码登陆&apos;) while 1: if &apos;http://&apos; in self.web.title: break print(&apos;扫码登陆成功&apos;) time.sleep(2) self.web.get(&apos;https://user.qzone.qq.com/&#123;&#125;&apos;.format(self.__username)) cookie=&apos;&apos; for elem in self.web.get_cookies(): cookie+=elem[&quot;name&quot;]+&quot;=&quot;+ elem[&quot;value&quot;]+&quot;;&quot; self.cookies=cookie self.get_g_tk() #time.sleep(10) self.headers[&apos;Cookie&apos;]=self.cookies self.web.quit()if __name__==&apos;__main__&apos;: sp=Spider() sp.login() networkx 分析networkx分析大体第一步都是构建网络，在网络中分增加节点node和增加egde,networkx支持批量添加。该部分和自己数据集的方式有关。下面主要是网络构建成功之后的分析部分12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# -*- coding: utf-8 -*-\"\"\"Created on Wed Oct 4 22:23:27 2017@author: x\"\"\"import networkx as nximport matplotlib.pyplot as pltimport operator#basicinformation of a networkdef basic_info(G): f=open('basic_info.txt','w') f.write('网络节点数：') f.write(str(G.number_of_nodes()) + '\\n') f.write('网络边数：') f.write(str(G.size()) + '\\n') f.write('网络边加权和：') f.write(str(G.size(weight='weight')) + '\\n') scc=nx.strongly_connected_components(G)#返回强连通子图的list wcc=nx.weakly_connected_components(G)#返回弱连通子图的list print(\"弱连接: \") f.write('弱连接:'+'\\n') for c in wcc:# print (c) f.write(str(c)) f.write('\\n') print(\"强连接: \") f.write('强连接:'+'\\n') for s in scc:# print(str(s)) f.write(str(s)+',') f.write('\\n') f.write('有向图平均路径长度：') f.write(str(nx.average_shortest_path_length(G)) + '\\n') G=G.to_undirected() f.write('平均聚类系数：') f.write(str(nx.average_clustering(G)) + '\\n') f.write('平均路径长度：') f.write(str(nx.average_shortest_path_length(G)) + '\\n')def node_exist(G,node): if G.has_node(node): return True else : return Falsedef draw_ego_graph(G,character,hops=1,show_lables=True): \"\"\" Expecting a graph_from-gdf \"\"\" y=\"%s的%s代好友网络\"%(character,hops) #Get the Ego Gaph and Position ego=nx.ego_graph(G,character,hops) pos = nx.spring_layout(ego) plt.figure(figsize=(12,12)) plt.axis('off') # Draw nx.draw_networkx_edges(ego,pos,alpha=0.8,with_lables=True) nx.draw_networkx_nodes(ego,pos,with_lables =True,node_size=50,cmp=plt.cm.hot) if show_lables: nx.draw_networkx_labels(ego,pos) plt.title('&lt;C&gt;=&#123;&#125;'.format(y)) plt.show()#find top 10 key people def key_people(G): centrality=nx.degree_centrality(G) nx.set_node_attributes(G,'centrality',centrality) degrees =sorted(centrality.items(),key=operator.itemgetter(1),reverse =True) for item in degrees[0:10]:print(\"%s : %0.3f\"%item) key_people()函数用来寻找在网络中，对网络连通最大的节点。数据可以反映很多现实问题。draw_ego_graph()用来寻找子网络，单张网络往往非常大，(测试中，本数据集整个节点20w),研究并可视化子网络","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://yoursite.com/categories/Data-Analysis/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://yoursite.com/tags/Data-Analysis/"}],"author":"初来的xiaomu"},{"title":"练习项目-CET查分","slug":"CET四六级查分","date":"2017-08-13T04:39:04.000Z","updated":"2019-09-06T01:57:44.164Z","comments":true,"path":"2017/08/13/CET四六级查分/","link":"","permalink":"http://yoursite.com/2017/08/13/CET四六级查分/","excerpt":"","text":"10/03/2017 更新代理接口，此项目不再更新本软件下载地址下载地址：http://pan.baidu.com/s/1i4SCMfr 密码：5wrm 说明软件说明之前一直都是写的python爬虫，最后主要考虑到client运行的实际情况，与python GUI部分没有接触过，开发起来稍微难度比较大。因此选用Java作为开发语言。程序逻辑比较简单，就是细节比较多，修修补补。 使用说明因为是练手，开发时间也比较短，大概2~3天，程序考虑很不完善，但是目前遇到的问题，基本都修复了，如果在使用过程中，遇到问题可以直接评论留言(说了这么多还不是因为不会-&gt;-&gt;) 一般用户无需浏览以下部分，此部分记录开发工程中遇到的一些问题，方便以后别人或者自己参考 开发记录逻辑为了控制方便，程序采用MVC设计模式，数据接口最开始来源于学信网(学信网在频繁时访问会需要验证码)，正在添加99宿舍接口 Learn more Jsoup.jar 的使用 Swing 传参数 Swing 监听 日志 99宿舍解密不成功，java难以调用","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"练习","slug":"练习","permalink":"http://yoursite.com/tags/练习/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"selenium3环境配置及其他细节优化","slug":"python3-selenium3环境配置及其他细节优化","date":"2017-07-08T16:00:00.000Z","updated":"2017-10-07T05:26:31.000Z","comments":true,"path":"2017/07/09/python3-selenium3环境配置及其他细节优化/","link":"","permalink":"http://yoursite.com/2017/07/09/python3-selenium3环境配置及其他细节优化/","excerpt":"","text":"之前在windows使用的firefox和chrome都是绿色版，配置的selenium3一直没有成功，于是尝试着在Linux下配置，成功配置了！ 准备OS: Linux ubuntu 16.04python3:Python 3.5.2Firefox默认安装 下载安装selenium3确定操作系统架构 打开终端，输入sudo uname -p 若返回i686，证明为32为操作系统 若返回x86_64,证明为64位操作系统 安装selenium3 对于同时安装python2与python3的环境而言，采用 1python3 -m pip install selenium3 若提示权限不够，加上sudo，即 1sudo python3 -m pip install selenium3 下载geckodriver 若firefox版本过高，需要安装geckodriver 这里是下载地址 选择对应操作系统版本的下载，然后解压，并进入解压之后的目录，接着 安装geckodriver 记住要在解压之后的父目录操作 cp geckodriver /usr/local/bin/ 安装xvfbsudo apt-get install xvfb 安装pyvirtualdisplay1python3 -m pip install pyvirtualdisplay 测试selenium3 复制以下代码，保存为test.py123456789101112from selenium import webdriverimport timedriver = webdriver.Firefox()driver.get(\"http://www.baidu.com\")driver.find_element_by_id(\"kw\").send_keys(\"python开发\")driver.find_element_by_id(\"su\").click()time.sleep(5) #等待5s，关闭浏览器。driver.quit() 然后运行 若报错selenium.common.exceptions.WebDriverException: Message: &#39;geckodriver&#39; executable may have wrong permissions 定位到之前geckodriver的目录，设置权限为-rwxr-xr-x即sudo chmod 755 geckodriver 其他优化PYPI 更换国内源在安装过程中，发现默认的安装源太慢，可以通过更换国内源来提速。笔者这边较快的源是阿里源和清华源 WINDOWS更换找到python的安装目录，定位到\\Lib\\site-packages\\pip\\目录，然后打开cmdoptions.py，搜索index_url得到如下1234567891011index_url = partial( Option, '-i', '--index-url', '--pypi-url', dest='index_url', metavar='URL', default=PyPI.simple_url, help=\"Base URL of Python Package Index (default %default). \" \"This should point to a repository compliant with PEP 503 \" \"(the simple repository API) or a local directory laid out \" \"in the same format.\") 修改为以下：123456789101112131415index_url = partial( Option, '-i', '--index-url', '--pypi-url', dest='index_url', metavar='URL', #default=PyPI.simple_url, #清华源 #default='https://pypi.tuna.tsinghua.edu.cn/simple', #阿里源 default='https://mirrors.aliyun.com/pypi/simple/', help=\"Base URL of Python Package Index (default %default). \" \"This should point to a repository compliant with PEP 503 \" \"(the simple repository API) or a local directory laid out \" \"in the same format.\") 清华源和阿里源可以自己选择，只需要将另外的注释掉就可以了 Linux更换在/.pip/下新建或修改pip.conf为12[global]index-url = https://mirrors.aliyun.com/pypi/simple/ 注意在添加源的时候，要使用加密链接，即https。若使用http，需要添加–trusted-host这一参数以上方法是采用修改默认源的方法，还可以在安装的时候，指定安装源，使用pip -i参数.如1sudo python3 -m pip install selenium3 -i https://mirrors.aliyun.com/pypi/simple/ 同理，若使用http，需要添加–trusted-host这一参数 pip 升级所有package 1pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U 参考来源 采用pip 先列出outdate，再升级即 1python3 -m pip list --outdate --format=freeze &gt;&gt;update3.txt update3.txt为输出outdate的文件 打开update3.txt，去掉所有package后面的版本信息，也就是==以及以后的信息如pyOpenSSL==0.15.1 修改为pyOpenSSL 1python3 -m pip install -r update3.txt -U ubuntu清理垃圾 清理apt载安装软件缓存文件$sudo apt-get clean 清理期软件包$sudo apt-get autoclean 自卸载没软件依赖关系软件$sudo apt-get autoremove","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"毕业啦","slug":"重更","date":"2017-07-03T16:00:00.000Z","updated":"2017-10-07T05:26:19.000Z","comments":true,"path":"2017/07/04/重更/","link":"","permalink":"http://yoursite.com/2017/07/04/重更/","excerpt":"","text":"临近毕业，本站重新开始更新。毕业了python爬虫也很长一段时间没有进一步学习了，最近准备重新学习，期待自己以后可以坚持学习，并留下相关的脚印。请大神手下留情 计划学习以下内容 python3爬虫及相关 图像处理 机器学习 反馈若有问题，请直接留言，我会尽快回复不知道会不会有读者,,, 参考本页面在搭建过程中主要参考以下页面 Hexo官方文档 lss53","categories":[],"tags":[]}]}